# -*- coding: utf-8 -*-
"""Canny Edge Model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12kcS-nbaGGnZK14KCzhOrZw7wbcRipqH
"""

from google.colab import drive 
drive.mount('/content/gdrive')

dir_path = "gdrive/My Drive/Canny Edge Dataset"

import os
for dirname, _, filenames in os.walk(dir_path):
    for filename in filenames:
        print(os.path.join(dirname, filename))

from keras.models import Sequential
from tensorflow.keras.layers import Flatten,Dense,Dropout,MaxPool2D,Conv2D
import os

sz = 128
from tensorflow.keras import Sequential
model = Sequential()
model.add(Conv2D(32,(3,3),padding = 'same',input_shape=(sz,sz,1),activation = 'relu'))
model.add(MaxPool2D((2,2)))
model.add(Conv2D(64, (3, 3), activation='relu'))
model.add(MaxPool2D(pool_size=(2, 2)))

model.add(Flatten())
model.add(Dense(units=128, activation='relu'))
model.add(Dropout(0.40))
model.add(Dense(96,activation='relu'))
model.add(Dropout(0.40))
model.add(Dense(units=64, activation='relu'))
model.add(Dense(27,activation="softmax"))

model.summary()

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy']) # categorical_crossentropy for more than 2

dir_path = "gdrive/My Drive/Canny Edge Dataset"

sz = 128
from keras.preprocessing.image import ImageDataGenerator
train_path = dir_path + '/train'
train_datagen = ImageDataGenerator(
        rescale=1./255,
        shear_range=0.2,
        zoom_range=0.2,
        horizontal_flip=True)
training_set = train_datagen.flow_from_directory(train_path,
                                                 target_size=(sz, sz),
                                                 batch_size=10,
                                                 color_mode='grayscale',
                                                 class_mode='categorical')

sz = 128
test_datagen = ImageDataGenerator(rescale=1./255)
test_path = dir_path + '/test'
test_set = test_datagen.flow_from_directory(test_path,
                                            target_size=(sz , sz),
                                            batch_size=10,
                                            color_mode='grayscale',
                                            class_mode='categorical')

history = model.fit(
        training_set,
         # No of images in training set
        epochs=5,
        validation_data=test_set,
         verbose = 1)

print(history.history.keys())

history.history

import matplotlib.pyplot as plt
import numpy
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

model_json = model.to_json()
with open("model-tkdi2.json", "w") as json_file:
    json_file.write(model_json)
print('Model Saved')
model.save_weights('model-tkdi2.h5')
print('Weights saved')

from keras.models import model_from_json
json_file = open("model-tkdi2.json", "r")
model_json =json_file.read()
json_file.close()
bw_model = model_from_json(model_json)
bw_model.load_weights("model-tkdi2.h5")

